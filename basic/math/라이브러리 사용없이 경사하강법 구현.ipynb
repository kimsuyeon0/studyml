{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient function \n",
    "def difference_quotient(f,x,h): #함수의 변화율\n",
    "    return( f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시\n",
    "def square(x):\n",
    "    return x*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(x):\n",
    "    return 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x^2의 도함수가 2x라는 건 다 알아요.\n",
    "### 그런데 만약에 모른다고 가정했을 때,\n",
    "### 위에서 정의한 difference_quotient 이용해서 얼마나 정확하게 근사값을 구할 수 있는지 체크해보자!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_estimate=lambda x: difference_quotient(square, x, h=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = range(-10, 10)\n",
    "plt.title(\"Actual Derivatives vs Estimates\")\n",
    "plt.plot(x, list(map(derivative, x)), 'rx', label=\"Actual\")\n",
    "plt.plot(x,list(map(derivative_estimate, x)), 'b+', label=\"Estimate\")\n",
    "plt.legend(loc = 9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다변수 함수의 gradient의 근사값을 구하는 함수를 만들어보자!\n",
    "def partial_derviative_quotient(f,v,i,h):\n",
    "    \"\"\" 함수 f의 i번째 편도함수가 v(벡터)에서 가지는 값\"\"\"\n",
    "    w=[v_j + (h if j==i else 0)\n",
    "      for j, v_ in enumerate(v)]\n",
    "    return (f(w) - f(v)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f, a, h=0.0001):\n",
    "    return [partial_derviative_quotient(f,v,i,h)\n",
    "           for i, _ in enumerate(v)] # 인덱스만 받고 싶고 값은 안받고 싶을 때 뒤에 _를 쓴다. 반대도 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient 적용하기 !\n",
    "\n",
    "### 제곱합합수(sum of square)는 v(벡터)를 파라미터로 받았을 때, v=0벡터일때 가장 작은 값을 가져요!\n",
    "### 그런데, 만약에 이 사실을 모른다고 가정하고, 경사하강법을 이용해 3차원 벡터의 최솟값을 구해보자!\n",
    "## -----------------------\n",
    "#### 임의의 시작점을 잡고, gradient가 아주 작아질때까지 경사의 반대방향으로 조금씩 이동시킴!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(v, direction, step_size):\n",
    "    \"\"\"v 벡터를 step_size만큼 direction 방향으로 이동\"\"\"\n",
    "    return [v_i + step_size * direction_i \n",
    "            for v_i, direction_i in zip(v, direction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nex) v=[x1,x2,x3]라 하면\\n    sum_of_square =x1^2+x2^2+x3^2\\n    각 원소에 대해 편미분 적용 -> gradient =[2* x1, 2* x2, 2*x3]\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_of_squares_gradient(v):\n",
    "    return [2*v_i for v_i in v]\n",
    "\"\"\"\n",
    "ex) v=[x1,x2,x3]라 하면\n",
    "    sum_of_square =x1^2+x2^2+x3^2\n",
    "    각 원소에 대해 편미분 적용 -> gradient =[2* x1, 2* x2, 2*x3]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 임의의 시작점을 잡자\n",
    "import random\n",
    "random.seed(12345)\n",
    "v=[random.randint(-10,10) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance=0.000001 #근사값과 참값의 차이가 이것보다 작으면 경사하강법 종료 (기준값)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def distance(x,y): #두 점 사이의 거리 구하기\n",
    "    a=0\n",
    "    for x_i, y_i in zip(x,y):\n",
    "        a += (x_i-y_i) **2\n",
    "    return math.sqrt(a) # sqrt : 제곱근 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance([3,4],[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. 제곱합 함수의 경사하강법 과정\n",
    "while True:\n",
    "    gradient = sum_of_squares_gradient(v) #v의 gradient를 계산\n",
    "    next_v=step(v, gradient, -0.01) #원래 gradient의 반대방향으로 이동\n",
    "    if distance(next_v, v) < tolerance :\n",
    "        break\n",
    "    v=next_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4161909964880136e-06, -4.7206366549600495e-06, -4.7206366549600445e-07]\n"
     ]
    }
   ],
   "source": [
    "print(v) #totlerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 적절한 이동거리 구하기\n",
    "#### 경사하강법 적용 시, step_size를 정하는 방법이 몇 가지 있다.\n",
    "\n",
    "##### 1) 이동 거리를 항상 고정\n",
    "##### 2) 시간에 따라 이동거리를 점차 줄여나감\n",
    "##### 3) 이동할 때마다, 목적함수 최소화하는 이동거리로 정함.\n",
    "\n",
    "###### 마지막이 제일 좋아보이는데, 계산 비용이 크기 때문에, 몇 몇 정해진 이동 거리를 시도해보고, 그 중 목적함수를 가장 최소화하는 값을 고르는 방법이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_sizes=[100,10,1,0.1,0.01,0.001,0.0001,0.00001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe(f):\n",
    "    \"\"\"f와 똑같은 함수를 반환하지만 f에 오류가 발생하면 무한대를 반환\"\"\"\n",
    "    def sate_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf') # 무한대 표기법\n",
    "    return safe_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    # batch : 반복문 돌 때마다 데이터셋 전체를 살펴본다.\n",
    "    \"\"\"\n",
    "    target_fn : 경사하강법 적용하려는 함수\n",
    "    gradient_fn : target_fn의 gradient 함수\n",
    "    theta_0 :시작점\n",
    "    totlerance : 기준\n",
    "    \"\"\"\n",
    "    step_sizes=[100,10,1,0.1,0.01,0.001,0.0001,0.00001]\n",
    "    theta=theta_0 #처음에는 theta를 시작점으로 설정!\n",
    "    target_fn=safe(target_fn) #오류를 처리할 수 있는 target_fn으로 변환!\n",
    "    value=target_fn(theta) #value : 최소화 하려는 값\n",
    "    \n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas=[step(theta, gradient, -step_size)\n",
    "                    for step_size in step_sizes]\n",
    "        \n",
    "        #함수를 최소화 시키는 theta 선택 !\n",
    "        next_theta=min(next_thetas, key=target_fn) # target_fn을 최소로 하는 next_theta 값을 사용.\n",
    "        next_value=target_fn(next_theta)\n",
    "        \n",
    "        #tolerance만큼 수렴하면 멈춤\n",
    "        if abs(value-next_value)<tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value=next_theta, next_value\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위의 minimize_batch적용하면, 반복문 돌 때마다 데이터 전체의 gradient값을 계산하기 때문에 시간이 오래 걸림\n",
    "### 그런데, 대부분의 오류 함수는 더할 수 있는 속성ㅇ르 갖고 있다.\n",
    "### 즉, 데이터 전체에 대한 오류값이 각 데이터 포인트의 오류값의 합과 같다\n",
    "\n",
    "#### 이럴 때는, 한 번 반복문을 돌 때마다 데이터 포인트 1개에 대한 gradient를 계산하는 SGD(확률적 경사하강법)\n",
    "#### SGD는 수렴할 때까지 전체 데이터셋을 반복적으로 사용, 한번 반복문 돌때다마 임의의 순서대로 데이터포인트를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_random_ordef(data):\n",
    "    \"\"\"\n",
    "    임의의 순서대로 data의 데이터 포인트를 반환\n",
    "    \"\"\"\n",
    "    indexs = [i for i, _ in enumerate(data)] #데이터 포인트의 index를 list로 반환\n",
    "    random.shuffle(indexes) # 인덱스를 섞고 \n",
    "    for i in indexes:\n",
    "        yield data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_subtract(v, w):\n",
    "    \"\"\"각 성분끼리 뺀다\"\"\"\n",
    "    return [v_i - w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def scalar_multiply(c,v):\n",
    "    \"\"\"c : 숫자, v:벡터\"\"\"\n",
    "    return [c*v_i for v_i in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 데이터 포인트에 대해 gradient를 계산\n",
    "# 최적해 근방에서 영원히 떠돌게 될 수가 있기 때문에 함수값이 한 동안 줄지 않으면 이동 거리를 줄이고 알고리즘을 종료\n",
    "\n",
    "def minimize_stochastic(target_fn, gradient_fn,x,y, theta_0, alpha_0=0.01):\n",
    "    \n",
    "    data=zip(x,y)\n",
    "    theta=theta_0 # 첫 시작점\n",
    "    alpha=alpha_0 # 기본 이동거리 : 0.01\n",
    "    min_theta, min_value=None, float(\"inf\") #시작할 때의 최솟값\n",
    "    iterations_with_no_improvement=0\n",
    "    \n",
    "    #만약 100번 넘게 반복하는 동안 더 작아지지 않으면 멈춤\n",
    "    while iterations_with_no_improvement<100:\n",
    "        value =sum (target_fn (x_i, y_i, theta) for x_i, y_i in data)\n",
    "        \n",
    "        if value<min_value:\n",
    "            #새로운 최솟값을 찾았다면 이 값을 저장하고 기본 이동거리로 다시 돌아감\n",
    "            min_theta, min_value=theta, value\n",
    "            iterations_with_no_imporvement=0\n",
    "            alpha=alpha_0\n",
    "        else :\n",
    "            #만약 최솟값이 줄어들지 않는다면 이동 거리를 축소\n",
    "            iterations_with_no_improvement +=1\n",
    "            alpha *=0.9\n",
    "            \n",
    "        #각 데이터 포인트에 대해 경사를 계산\n",
    "        for x_i,y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta=vectorPsubtract(theta, scalar_multiply(alpha, gradient_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
